{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96c2e5d-a7f2-40b6-81b1-2070ae461990",
   "metadata": {},
   "source": [
    "### Tecnolgías del Lenguaje. Entregable 4\n",
    "---\n",
    "#### Reducción del número de posts por usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdee96-571e-49cd-8987-951d61bb3d6f",
   "metadata": {},
   "source": [
    "El dataset original contiene un número muy variable de publicaciones por usuario: algunos autores escriben miles de posts, mientras que otros solo unos pocos. Esto puede introducir un desequilibrio importante en los análisis posteriores, ya que los usuarios más prolíficos dominarían los modelos y sesgarían los resultados. Además, **el tiempo de cómputo es un cuello de botella** en este problema: lo hemos comprobado en las primeras aproximaciones a la solución, cuando pretendíamos utilizar un modelo transformer para seleccionar las frases (dentro de cada post) más similares a las frases que se utilizaron en el cuestionario del Big-5. Para mitigar este problema, se ha aplicado un proceso de reducción simple y controlada:\n",
    "- Se cargan los datos agrupados por usuario, donde cada fila contiene el nombre del autor y una lista con todos sus posts.\n",
    "- Se seleccionan, para cada usuario, como máximo 10 publicaciones, escogiendo aquellas de mayor longitud (en número de caracteres).\n",
    "- La longitud del texto se usa como criterio porque los posts más extensos suelen contener más información lingüística y psicológica relevante para el análisis de rasgos de personalidad. No es una solución muy acertada, pero recortará considerablemente los tiempos de ejecución.\n",
    "\n",
    "Finalmente, se guarda el nuevo dataset reducido, que mantiene una representación equilibrada y rica en contenido de cada usuario, pero con un tamaño total mucho más manejable para el procesamiento posterior. No es la solución que utilizaremos finalmente, pero puede servir como *baseline*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04aa9d3-ac7d-455d-9f41-9f74a75e6dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset reducido guardado en material/post_by_author_reducido.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = \"material/post_by_author.csv\"\n",
    "output_path = \"material/post_by_author_reducido.csv\"\n",
    "\n",
    "# Cargar dataset agrupado por usuario\n",
    "df = pd.read_csv(input_path, quoting=1)\n",
    "\n",
    "# Convertir la columna 'body' de string a lista\n",
    "import ast\n",
    "df['body'] = df['body'].apply(ast.literal_eval)\n",
    "\n",
    "# Función para quedarnos con los 10 posts más largos\n",
    "def top_n_longest(posts, n=10):\n",
    "    # Ordenar los posts por longitud (número de caracteres) de mayor a menor\n",
    "    posts_sorted = sorted(posts, key=len, reverse=True)\n",
    "    # Quedarse con los n primeros\n",
    "    return posts_sorted[:n]\n",
    "\n",
    "# Aplicar a cada fila\n",
    "df['body'] = df['body'].apply(lambda posts: top_n_longest(posts, n=10))\n",
    "\n",
    "# Guardar CSV reducido\n",
    "df.to_csv(output_path, index=False, quoting=1)\n",
    "\n",
    "print(f\"✅ Dataset reducido guardado en {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
