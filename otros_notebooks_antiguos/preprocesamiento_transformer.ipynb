{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bca3345-1986-421c-b9dd-3128d9808c26",
   "metadata": {},
   "source": [
    "### Tecnolg√≠as del Lenguaje. Entregable 4\n",
    "---\n",
    "#### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a472fd2-2a92-4f5b-b4d3-89b1527a76ce",
   "metadata": {},
   "source": [
    "Dividir los posts en oraciones permite un an√°lisis ling√º√≠stico m√°s preciso y granular, ya que las oraciones son unidades sem√°nticas m√°s estables que los textos completos. En el procesamiento del lenguaje natural, trabajar a nivel de oraci√≥n facilita tareas como la detecci√≥n de rasgos de personalidad, el an√°lisis sint√°ctico o la extracci√≥n de caracter√≠sticas estil√≠sticas, evitando que la longitud o la complejidad de los textos afecten al modelo. Adem√°s, al segmentar los posts, se pueden identificar patrones de escritura y estilo m√°s coherentes y comparables entre distintos autores, mejorando la calidad y la interpretabilidad de los resultados en el an√°lisis posterior.\n",
    "\n",
    "Por ello, en este notebook realizamos la **segmentaci√≥n de los posts por oraciones**, transformando cada entrada textual en una lista de frases individuales. Este paso prepara el dataset para etapas posteriores de an√°lisis ling√º√≠stico y de personalidad, en las que cada oraci√≥n podr√° ser tratada como una unidad de observaci√≥n independiente. De esta forma, se facilita la extracci√≥n de rasgos ling√º√≠sticos espec√≠ficos, la aplicaci√≥n de modelos de NLP basados en contexto y la identificaci√≥n de patrones de escritura m√°s finos y consistentes entre los distintos usuarios.\n",
    "\n",
    "Queremos transformar el dataset original en uno que contenga una fila por usuario y una lista para cada usuario que almacene las oraciones de todos sus posts. El objetivo es preparar un dataset que sirva para entrenar un modelo Transformer con el que reduzcamos el gran conjunto de datos original al conjunto de datos de inter√©s, esto es, aquel que **almacena las frases de cada usuario que son m√°s representativas de cada rasgo Big-5**.\n",
    "\n",
    "- **Input: material proporcionado en bruto.**\n",
    "- **Output: material preprocesado, preparado para ser procesado por un Transformer.**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad1000f-8135-4ffb-8acd-74a386eeeca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisitos\n",
    "# pip install pandas emoji ftfy clean-text emot\n",
    "import re\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from ftfy import fix_text\n",
    "from cleantext import clean\n",
    "from emot.emo_unicode import EMOTICONS  # para emoticonos tipo :-) :P\n",
    "\n",
    "# Mapa de s√≠mbolos/pictogramas extra\n",
    "SYMBOL_MAP = {\n",
    "    '‚ô¨': 'music', '‚ô™': 'musical_note', '‚ô´': 'melody',\n",
    "    '‚ô•': 'heart', '‚ù§': 'heart', 'üíï': 'hearts', 'üíî': 'broken_heart',\n",
    "    '‚ú®': 'shine', '‚≠ê': 'star', 'üåü': 'star',\n",
    "    '‚úî': 'check', '‚úñ': 'cross', '‚Ä¶': 'ellipsis'\n",
    "}\n",
    "\n",
    "# Construir un mapa de emoticonos (texto) a tokens legibles\n",
    "EMOTICON_MAP = {}\n",
    "for k, v in EMOTICONS.items():\n",
    "    # EMOTICONS keys are regex-like; normalizamos claves simples\n",
    "    safe_key = re.sub(r'[^A-Za-z0-9]', '_', k)[:40]\n",
    "    EMOTICON_MAP[k] = v.replace(',', '').replace(':','').strip().replace(' ', '_')\n",
    "\n",
    "# Rango general para s√≠mbolos/pictogramas unicode (capturamos los que no est√°n en emoji.demojize)\n",
    "UNICODE_SYMBOLS_RE = re.compile(\n",
    "    \"[\" \n",
    "    \"\\U0001F300-\\U0001F5FF\"  # s√≠mbolos y pictogramas\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transporte y s√≠mbolos varios\n",
    "    \"\\u2600-\\u26FF\"          # misc symbols (‚ô¨, ‚ù§ etc)\n",
    "    \"]+\", flags=re.UNICODE\n",
    ")\n",
    "\n",
    "URL_RE = re.compile(r'(https?://\\S+|www\\.\\S+)')\n",
    "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b')\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "HASHTAG_RE = re.compile(r'#(\\w+)')\n",
    "MULTI_PUNCT_RE = re.compile(r'([!?.]){2,}')  # !!! o ???\n",
    "NON_PRINTABLE_RE = re.compile(r'[\\x00-\\x1f\\x7f-\\x9f]')\n",
    "EXCESS_WS_RE = re.compile(r'\\s+')\n",
    "\n",
    "def replace_symbol_tokens(text):\n",
    "    for s, w in SYMBOL_MAP.items():\n",
    "        text = text.replace(s, f' {w} ')\n",
    "    return text\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    # Las claves de EMOTICONS keys no siempre se pillan\n",
    "    for emoticon, meaning in EMOTICON_MAP.items():\n",
    "        # intenta reemplazar directamente los emoticones ascii del diccionario de EMOTICONS\n",
    "        if emoticon in text:\n",
    "            text = text.replace(emoticon, f' {meaning} ')\n",
    "    # fallback: captura patrones simples como :-), :P, XD, :(\n",
    "    text = re.sub(r'(:-\\)|:\\)|:\\]|=+\\)|:D|XD|xD)', ' :smiling_face ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(:-\\(|:\\()', ' :sadness_face ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(:P|:p|;P|;p)', ' :sticking_out_tongue ', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def demojize_and_map(text):\n",
    "    # demojize convierte emojis a :emoji_name:\n",
    "    text = emoji.demojize(text, language='es')  # devuelve :smiling_face:\n",
    "    # convertir :emoji_name: -> emoji_name (sin dos puntos, con guiones -> guiones)\n",
    "    text = re.sub(r':([a-zA-Z0-9_+-]+):', lambda m: ' ' + m.group(1).replace('_', ' ') + ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_junk_chars(text):\n",
    "    # reemplaza caracteres no imprimibles y secuencias de puntuaci√≥n repetidas (reduce '!!!' a ' ! ')\n",
    "    text = NON_PRINTABLE_RE.sub(' ', text)\n",
    "    text = MULTI_PUNCT_RE.sub(lambda m: ' ' + m.group(1) + ' ', text)\n",
    "    return text\n",
    "\n",
    "def clean_post(text, lower=True, keep_hashtags=False):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # unicode fixes (arregla mojibake y cosas raras)\n",
    "    text = fix_text(text)\n",
    "    # quitar URLs y correos\n",
    "    text = URL_RE.sub(' ', text)\n",
    "    text = EMAIL_RE.sub(' ', text)\n",
    "    # quitar menciones\n",
    "    text = MENTION_RE.sub(' ', text)\n",
    "    # convertir hashtags a la palabra (ej: #amor -> amor) o borrarlos\n",
    "    if keep_hashtags:\n",
    "        text = HASHTAG_RE.sub(r'\\1', text)\n",
    "    else:\n",
    "        text = HASHTAG_RE.sub(' ', text)\n",
    "    # primero demojize (cubre la mayor√≠a)\n",
    "    text = demojize_and_map(text)\n",
    "    # luego mapea s√≠mbolos extra (‚ô¨ ‚ù§ ...)\n",
    "    text = replace_symbol_tokens(text)\n",
    "    # emoticonos de texto\n",
    "    text = replace_emoticons(text)\n",
    "    # elimina cualquier pictograma restante\n",
    "    text = UNICODE_SYMBOLS_RE.sub(' ', text)\n",
    "    # quitar caracteres basura y normalizar espacios\n",
    "    text = remove_junk_chars(text)\n",
    "    text = EXCESS_WS_RE.sub(' ', text).strip()\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Se mantiene por defecto no eliminar n√∫meros ni toda la puntuaci√≥n para no perder se√±ales.\n",
    "    text = clean(text, fix_unicode=False, to_ascii=False, lower=False, no_urls=True, no_emails=True,\n",
    "                 no_phone_numbers=True, no_numbers=False, no_punct=False)\n",
    "    # √∫ltimo barrido de espacios\n",
    "    text = EXCESS_WS_RE.sub(' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_dataframe(df, text_col='body', inplace=False, **kwargs):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    df['clean_' + text_col] = df[text_col].fillna('').apply(lambda x: clean_post(x, **kwargs))\n",
    "    return df\n",
    "\n",
    "# Uso:\n",
    "df = pd.read_csv('material/posts.csv')  # tu CSV\n",
    "df = clean_dataframe(df, text_col='body', inplace=False, lower=True, keep_hashtags=True)\n",
    "df.to_csv('material/posts_clean.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fdb588-fe1b-4c1d-83f0-282cb7cd420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                         clean_body\n",
      "0   -Areopagan-  [your first and second question is the same qu...\n",
      "1     -BigSexy-  [i've been asked to cum everywhere with my ex ...\n",
      "2    -BlitzN9ne  [i'm currently in the middle of making a payda...\n",
      "3  -CrestiaBell  [first and foremost i extend my condolences to...\n",
      "4        -dyad-  [i failed both . i'm great at reading people i...\n"
     ]
    }
   ],
   "source": [
    "# Asegurarse de que las columnas est√©n bien nombradas\n",
    "df.columns = df.columns.str.strip()  # elimina espacios si los hubiera\n",
    "\n",
    "# Limpiar NaN o textos vac√≠os\n",
    "df['clean_body'] = df['clean_body'].fillna('').astype(str)\n",
    "\n",
    "# Agrupar por autor\n",
    "df_grouped = (\n",
    "    df.groupby('username', as_index=False)\n",
    "      .agg({'clean_body': list})   # convierte los posts de cada autor en lista\n",
    ")\n",
    "\n",
    "# Ahora cada fila tiene una lista de strings\n",
    "print(df_grouped.head())\n",
    "df_grouped.to_csv('material/posts_clean_by_author.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e360c06-c54f-4aa9-82f8-8f66152a10aa",
   "metadata": {},
   "source": [
    "Tenemos los posts en una lista. Solo necesitamos dividir los posts de cada usuario en oraciones, para posteriormente quedarnos con las m√°s representativas.\n",
    "\n",
    "üìù **Nota**: Al principio, utilizamos el m√≥dulo `spacy`, pero al tener m√°s de 3 millones de posts la ejecuci√≥n era demasiado larga. La siguiente soluci√≥n es m√°s r√°pida, pero menos precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67cefe32-ecba-4685-8fb4-1203f534d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       username                                         clean_body\n",
      "0   -Areopagan-  [your first and second question is the same qu...\n",
      "1     -BigSexy-  [i've been asked to cum everywhere with my ex ...\n",
      "2    -BlitzN9ne  [i'm currently in the middle of making a payda...\n",
      "3  -CrestiaBell  [first and foremost i extend my condolences to...\n",
      "4        -dyad-  [i failed both ., i'm great at reading people ...\n"
     ]
    }
   ],
   "source": [
    "def quick_split(posts):\n",
    "    sentences = []\n",
    "    for post in posts:\n",
    "        parts = re.split(r'(?<=[.!?])\\s+', post.strip())\n",
    "        sentences.extend([p for p in parts if p])\n",
    "    return sentences\n",
    "\n",
    "df_grouped['clean_body'] = df_grouped['clean_body'].apply(quick_split)\n",
    "\n",
    "# Ahora cada fila tiene una lista de oraciones para cada autor\n",
    "print(df_grouped.head())\n",
    "df_grouped.to_csv('material/posts_clean_by_author.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92a7be-20fe-4219-b250-73a4e55aa77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
