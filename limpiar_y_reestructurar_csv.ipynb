{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d861dca-501f-49ac-886a-815c5999668a",
   "metadata": {},
   "source": [
    "### Tecnolgías del Lenguaje. Entregable 4\n",
    "---\n",
    "#### Corrección del archivo CSV original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8f6fe-062c-4a07-b016-fe10cf9356d4",
   "metadata": {},
   "source": [
    "El dataset original `material/posts.csv` contiene dos columnas: `username` y `body`, donde `body` corresponde al texto completo de cada publicación. Sin embargo, hemos detectado que el archivo presenta inconsistencias en el uso de comillas, ya que algunos posts aparecen entrecomillados y otros no. Esto provoca errores al leer el archivo con `pandas.read_csv()`, ya que el separador de coma se confunde con las comas internas del texto, generando filas con más de dos columnas o registros mal alineados.\n",
    "\n",
    "Para resolver este problema, se utiliza el módulo estándar `csv` de Python, que permite reescribir el archivo garantizando un formato CSV válido y seguro. El código realiza las siguientes operaciones:\n",
    "- Lectura del archivo original (`material/posts.csv`) con el lector csv.reader, tolerando posibles errores de codificación.\n",
    "- Creación de un nuevo archivo corregido (`material/posts_corregido.csv`), utilizando `csv.writer` con la opción `quoting=csv.QUOTE_ALL`. Esta opción fuerza que todas las celdas se escriban entre comillas dobles, escapando automáticamente las comillas internas (`\"` → `\"\"`) según el **estándar CSV (RFC 4180)**.\n",
    "- Reparación de filas mal formateadas: Si una fila tiene más de dos columnas, se asume que el texto del post contenía comas no entrecomilladas;\n",
    "por tanto, se recombinan todas las columnas a partir de la segunda (`\",\".join(row[1:])`) para reconstruir el contenido completo del post. Si la fila tiene exactamente dos columnas, se escribe tal cual. Si la fila está vacía o corrupta, se omite.\n",
    "- Escritura del archivo corregido, que ahora tiene exactamente dos columnas por línea (`username`, `body`), con las comillas y los caracteres especiales correctamente escapados.\n",
    "\n",
    "El resultado es un archivo estructuralmente coherente, legible por pandas sin errores y que conserva íntegramente el contenido textual original de los posts. Es útil almacenarlo porque puede servir de *checkpoint* en nuevas soluciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4f8a43-67e6-4a2f-a2c7-7456063b045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV corregido guardado en material/posts_corregido.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_path = \"material/posts.csv\"\n",
    "output_path = \"material/posts_corregido.csv\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as fin, \\\n",
    "     open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "    \n",
    "    reader = csv.reader(fin)\n",
    "    writer = csv.writer(fout, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    for row in reader:\n",
    "        # Si la línea tiene más de dos columnas, las juntamos\n",
    "        if len(row) > 2:\n",
    "            username = row[0]\n",
    "            body = \",\".join(row[1:])  # volver a unir texto roto por comas\n",
    "            writer.writerow([username, body])\n",
    "        elif len(row) == 2:\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            # si por error hay líneas vacías, las saltamos\n",
    "            continue\n",
    "\n",
    "print(f\"✅ CSV corregido guardado en {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb864ee-2704-45c6-bae0-45cb4e536447",
   "metadata": {},
   "source": [
    "Algunos posts son demasiado largos y el modelo SBERT no podrá computar toda la entrada simultáneamente. Por ello, decidimos **dividir los posts más largos para adaptarlos al límite de la entrada del modelo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8415dbd-5995-44d8-9d4a-ef3ee2cf7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_PATH = \"material/posts_corregido.csv\"\n",
    "OUTPUT_PATH = \"material/posts_corregido_dividido.csv\"\"\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "# Detectar límites\n",
    "MODEL_MAX = tokenizer.model_max_length\n",
    "DESIRED_MAX = 256\n",
    "try:\n",
    "    special = tokenizer.num_special_tokens_to_add(pair=False)\n",
    "except Exception:\n",
    "    special = 2\n",
    "\n",
    "CHUNK_SIZE = min(DESIRED_MAX, MODEL_MAX - special)\n",
    "OVERLAP = 50\n",
    "\n",
    "\n",
    "def split_text_on_tokens_safe(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    n = len(token_ids)\n",
    "    if n <= chunk_size:\n",
    "        return [text.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap if (0 < overlap < chunk_size) else chunk_size\n",
    "    if step <= 0:\n",
    "        step = chunk_size\n",
    "\n",
    "    for start in range(0, n, step):\n",
    "        end = start + chunk_size\n",
    "        chunk_ids = token_ids[start:end]\n",
    "        chunk_text = tokenizer.decode(\n",
    "            chunk_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        ).strip()\n",
    "\n",
    "        if chunk_text:\n",
    "            chunks.append(chunk_text)\n",
    "        if end >= n:\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- PROCESAMIENTO CON BARRA DE PROGRESO ---\n",
    "# Primero contamos cuántas filas tiene el CSV\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\", errors=\"replace\") as fin:\n",
    "    total_lines = sum(1 for _ in fin)\n",
    "\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\", errors=\"replace\") as fin, \\\n",
    "     open(OUTPUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    reader = csv.reader(fin)\n",
    "    writer = csv.writer(fout, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    writer.writerow([\"username\", \"fragment_index\", \"fragment_text\"])\n",
    "\n",
    "    for row in tqdm(reader, total=total_lines, desc=\"Procesando\"):\n",
    "        if not row:\n",
    "            continue\n",
    "\n",
    "        if len(row) > 2:\n",
    "            username = row[0]\n",
    "            body = \",\".join(row[1:])\n",
    "        elif len(row) == 2:\n",
    "            username, body = row\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        fragments = split_text_on_tokens_safe(body)\n",
    "\n",
    "        for i, frag in enumerate(fragments):\n",
    "            writer.writerow([username, i, frag])\n",
    "\n",
    "print(\"✅ Listo. Archivo generado en:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f760d-9550-4bbc-949b-d92c1947051b",
   "metadata": {},
   "source": [
    "Nuestro objetivo final es realizar predicciones sobre los valores Big-5 (openness, conscientiousness, extraversion, agreeableness, neuroticism) **para un autor (username)** de posts. Por tanto, debemos agrupar los posts por usuario.\n",
    "\n",
    "En este paso preparamos el dataset para análisis de similitud semántica: **cada fila corresponde a un usuario único** y **la columna body contiene una lista de strings, cada string es un post de ese usuario**. Para ello:\n",
    "- Cargamos el CSV corregido que acabamos de crear, `posts_corregido.csv`.\n",
    "- Agrupamos los posts por `username`.\n",
    "- Limpiamos posibles valores NaN dentro de las listas de posts.\n",
    "- Verificamos que la estructura es correcta:\n",
    "    - Columnas: `username`, `body`, donde cada `body` es una lista de strings.\n",
    "- Guardamos el CSV final post_by_author.csv listo para procesamiento con modelos de NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354e0eb4-da8b-4936-9875-d25c067e4659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas: ['username', 'body']\n",
      "Primeros registros:\n",
      "        username                                               body\n",
      "0   -Areopagan-  [Your first and second question is the same qu...\n",
      "1     -BigSexy-  [I've been asked to cum everywhere with my ex ...\n",
      "2    -BlitzN9ne  [I'm currently in the middle of making a Payda...\n",
      "3  -CrestiaBell  [First and foremost I extend my condolences to...\n",
      "4        -dyad-  [I failed both...I'm great at reading people i...\n",
      "Tipo 'body' de la primera fila: <class 'list'>\n",
      "Primer post de la primera fila: Your first and second question is the same question. I'll try to make it more incisive for you because you don't articulate what you want to know. Do I think people who cooperate also compete? Yeah, sure, obviously, in a separate way people who play chess, or style themselves in order to be attractive, merely agree that there is a game and the game has rules. Fine. I don't think they cooperate in terms of winning the game because if I know, especially because you've told me, what you're planning that is self-defeating. It's mere agreement to a definition, it's not the point of the game, the point is to win. My approach to life isn't centered on men or maleness (that's what androcentric means), I am a man so I have to focus on myself but I care about things most people care about regardless of owning a penis. Hard to tell if you're asking me if I'm being self-centered or not because I don't know how I could be focused on men as an 'approach to life'. Mining is androcentric because that industry relies on men doing the majority of the labor if that makes the word usage clearer. \n",
      "Todos los usernames son cadenas de texto?: True\n",
      "Todas las filas de 'body' son listas de strings?: True\n",
      "✅ Dataset agrupado por usuario guardado en material/post_by_author.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "input_path = \"material/posts_corregido_dividido.csv\"\n",
    "output_path = \"material/posts_by_author.csv\"\n",
    "\n",
    "# Cargar CSV corregido\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Agrupar posts por usuario y convertir a lista de strings\n",
    "grouped = df.groupby('username')['body'].apply(list).reset_index()\n",
    "\n",
    "# Limpiar posibles NaN dentro de las listas de posts\n",
    "grouped['body'] = grouped['body'].apply(lambda posts: [p for p in posts if isinstance(p, str) and not (isinstance(p, float) and math.isnan(p))])\n",
    "\n",
    "# Verificación rápida\n",
    "print(\"Columnas:\", grouped.columns.tolist())\n",
    "print(\"Primeros registros:\\n\", grouped.head())\n",
    "print(\"Tipo 'body' de la primera fila:\", type(grouped.loc[0, 'body']))\n",
    "print(\"Primer post de la primera fila:\", grouped.loc[0, 'body'][0])\n",
    "\n",
    "# Comprobar que 'username' es string\n",
    "all_usernames_str = all(isinstance(u, str) for u in grouped['username'])\n",
    "print(\"Todos los usernames son cadenas de texto?:\", all_usernames_str)\n",
    "\n",
    "# Comprobar que 'body' es lista de strings\n",
    "all_bodies_correct = True\n",
    "for i, row in grouped.iterrows():\n",
    "    body = row['body']\n",
    "    # Pandas lee listas de CSV como strings, así que primero evaluamos a lista real\n",
    "    try:\n",
    "        posts = eval(body) if isinstance(body, str) else body\n",
    "    except:\n",
    "        posts = []\n",
    "    if not isinstance(posts, list) or not all(isinstance(p, str) for p in posts):\n",
    "        all_bodies_correct = False\n",
    "        print(f\"Fila {i} incorrecta:\")\n",
    "        print(\"username:\", row['username'])\n",
    "        print(\"body:\", body[:200])  # mostrar los primeros 200 caracteres\n",
    "        break\n",
    "\n",
    "print(\"Todas las filas de 'body' son listas de strings?:\", all_bodies_correct)\n",
    "\n",
    "# Guardar CSV\n",
    "grouped.to_csv(output_path, index=False, quoting=1)  # quoting=csv.QUOTE_ALL\n",
    "\n",
    "print(f\"✅ Dataset agrupado por usuario guardado en {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a243a67-4887-4def-bde0-3b85aec730f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
